{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing for classification model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('cmu_dataset_final.csv')\n",
    "\n",
    "# Preprocess the text\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "data['processed_plot'] = data['plot'].apply(preprocess_text)\n",
    "\n",
    "# Preprocess the genres\n",
    "data['genres'] = data['genre'].apply(lambda x: x.split('|'))\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(data['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word2vec embeddings\n",
    "word2vec_model = KeyedVectors.load('word2vec_model_from_cmu_utf8.bin')\n",
    "\n",
    "# Tokenize and pad the text sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['processed_plot'])\n",
    "sequences = tokenizer.texts_to_sequences(data['processed_plot'])\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = word2vec_model.vector_size\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "\n",
    "# Google news word2vec embeddings\n",
    "import gensim.downloader as api\n",
    "embeddings = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "embedding_dim2 = embeddings.vector_size\n",
    "embedding_matrix2 = np.zeros((len(word_index) + 1, embedding_dim2))\n",
    "for word, i in word_index.items():\n",
    "    if word in embeddings:\n",
    "        embedding_matrix2[i] = embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the search space, specify which hyperparameters to tune\n",
    "space = {\n",
    "    'batch_size': hp.quniform('batch_size', 35, 70, 1),\n",
    "    'epochs': hp.choice('epochs', [5, 7, 10]),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.2, 0.3),\n",
    "    'lstm_units': hp.quniform('lstm_units', 64, 256, 64),\n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'Nadam']),\n",
    "    'trainable': hp.choice('trainable', [True, False]),\n",
    "    'embedding': hp.choice('embedding', [\n",
    "        {\n",
    "            'embedding_matrix': embedding_matrix,\n",
    "            'embedding_dim': embedding_dim\n",
    "        },\n",
    "        {\n",
    "            'embedding_matrix': embedding_matrix2,\n",
    "            'embedding_dim': embedding_dim2\n",
    "        }\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Define objective function\n",
    "# This builds, trains and evaluates the model with the specified hyperparameters\n",
    "def objective(params):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, params['embedding']['embedding_dim'], embeddings_initializer=Constant(params['embedding']['embedding_matrix']), trainable=params['trainable']))\n",
    "    model.add(LSTM(int(params['lstm_units']), dropout=params['dropout_rate'], recurrent_dropout=params['dropout_rate']))\n",
    "    model.add(Dense(len(mlb.classes_), activation='sigmoid'))\n",
    "    \n",
    "    optimizer = params['optimizer']\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=int(params['batch_size']), epochs=params['epochs'], verbose=0, validation_data=(X_test, y_test))\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    return -f1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, genres_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_results = pd.DataFrame(trials.results)\n",
    "\n",
    "# Add the hyperparameters to the DataFrame\n",
    "for key in best.keys():\n",
    "    trial_results[key] = [trial['misc']['vals'][key] for trial in trials.trials]\n",
    "\n",
    "# Compute the F1 score from the loss\n",
    "trial_results['f1_score'] = -trial_results['loss']\n",
    "\n",
    "# Sort by F1 score\n",
    "trial_results = trial_results.sort_values('f1_score', ascending=False)\n",
    "\n",
    "print(trial_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
