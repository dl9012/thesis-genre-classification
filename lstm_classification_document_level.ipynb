{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Genre Classification with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "RANDOM_STATE = 1212\n",
    "\n",
    "cmu_data = pd.read_csv('cmu_data_final.csv')\n",
    "imdb_data = pd.read_csv('imdb_data_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "cmu_data['processed_plot'] = cmu_data['plot'].apply(preprocess_text)\n",
    "imdb_data['processed_plot'] = imdb_data['plot'].apply(preprocess_text)\n",
    "\n",
    "# Word2vec embeddings, trained on the CMU dataset\n",
    "#word2vec_model = KeyedVectors.load('word2vec_model_from_cmu_utf8.bin')\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad the text sequences\n",
    "cmu_tokenizer = Tokenizer()\n",
    "cmu_tokenizer.fit_on_texts(cmu_data['processed_plot'])\n",
    "cmu_sequences = cmu_tokenizer.texts_to_sequences(cmu_data['processed_plot'])\n",
    "word_index = cmu_tokenizer.word_index\n",
    "cmu_padded_sequences = pad_sequences(cmu_sequences, maxlen=300)\n",
    "\n",
    "# Label encoding\n",
    "cmu_data['genre'] = cmu_data['genre'].apply(lambda x: x.split('|'))\n",
    "cmu_mlb = MultiLabelBinarizer()\n",
    "cmu_genres_encoded = cmu_mlb.fit_transform(cmu_data['genre'])\n",
    "\n",
    "# Train-test split\n",
    "cmu_X_train, cmu_X_test, cmu_y_train, cmu_y_test = train_test_split(\n",
    "    cmu_padded_sequences, cmu_genres_encoded, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad the text sequences\n",
    "imdb_tokenizer = Tokenizer()\n",
    "imdb_tokenizer.fit_on_texts(imdb_data['processed_plot'])\n",
    "imdb_sequences = imdb_tokenizer.texts_to_sequences(imdb_data['processed_plot'])\n",
    "word_index = imdb_tokenizer.word_index\n",
    "imdb_padded_sequences = pad_sequences(imdb_sequences, maxlen=300)\n",
    "\n",
    "# Label encoding\n",
    "imdb_data['genre'] = imdb_data['genre'].apply(lambda x: x.split('|'))\n",
    "imdb_mlb = MultiLabelBinarizer()\n",
    "imdb_genres_encoded = imdb_mlb.fit_transform(imdb_data['genre'])\n",
    "\n",
    "# Train-test split\n",
    "imdb_X_train, imdb_X_test, imdb_y_train, imdb_y_test = train_test_split(\n",
    "    imdb_padded_sequences, imdb_genres_encoded, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Create the word embedding matrix\n",
    "cmu_embedding_dim = word2vec_model.vector_size\n",
    "cmu_embedding_matrix = np.zeros((len(word_index) + 1, cmu_embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        cmu_embedding_matrix[i] = word2vec_model.wv[word] \"\"\"\n",
    "\n",
    "# Create the word embedding matrix\n",
    "cmu_embedding_dim = word2vec_model.vector_size\n",
    "cmu_embedding_matrix = np.zeros((len(word_index) + 1, cmu_embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:  # Change this line\n",
    "        cmu_embedding_matrix[i] = word2vec_model[word]  # And this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 656ms/step - accuracy: 0.3285 - loss: 0.5710 - val_accuracy: 0.4786 - val_loss: 0.4802\n",
      "Epoch 2/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 635ms/step - accuracy: 0.5089 - loss: 0.4744 - val_accuracy: 0.5310 - val_loss: 0.4494\n",
      "Epoch 3/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 635ms/step - accuracy: 0.5513 - loss: 0.4518 - val_accuracy: 0.5397 - val_loss: 0.4484\n",
      "Epoch 4/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 632ms/step - accuracy: 0.5653 - loss: 0.4359 - val_accuracy: 0.6026 - val_loss: 0.4140\n",
      "Epoch 5/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 625ms/step - accuracy: 0.6033 - loss: 0.4129 - val_accuracy: 0.6402 - val_loss: 0.3878\n",
      "Epoch 6/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 625ms/step - accuracy: 0.6185 - loss: 0.4111 - val_accuracy: 0.6341 - val_loss: 0.3922\n",
      "Epoch 7/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 629ms/step - accuracy: 0.6358 - loss: 0.3953 - val_accuracy: 0.6349 - val_loss: 0.3917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13ed4315750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, cmu_embedding_dim, embeddings_initializer=Constant(\n",
    "    cmu_embedding_matrix), trainable=False))\n",
    "model.add(LSTM(192, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(len(cmu_mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(cmu_X_train, cmu_y_train, batch_size=64, epochs=7, validation_data=(\n",
    "    cmu_X_test, cmu_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "cmu_y_pred = model.predict(cmu_X_test)\n",
    "cmu_y_pred_classes = (cmu_y_pred > 0.5).astype(int) #0.5 här är att den uppskattade sannolikheten måste vara över 0.5. Annars blir den None (gör stor inpact på precision/recall men är oftast en trade off mellan dom båda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Create the word embedding matrix\n",
    "imdb_embedding_dim = word2vec_model.vector_size\n",
    "imdb_embedding_matrix = np.zeros((len(word_index) + 1, imdb_embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        imdb_embedding_matrix[i] = word2vec_model.wv[word] \"\"\"\n",
    "\n",
    "# Create the word embedding matrix\n",
    "imdb_embedding_dim = word2vec_model.vector_size\n",
    "imdb_embedding_matrix = np.zeros((len(word_index) + 1, imdb_embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:  # Change this line\n",
    "        imdb_embedding_matrix[i] = word2vec_model[word]  # And this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 582ms/step - accuracy: 0.3423 - loss: 0.5653 - val_accuracy: 0.5668 - val_loss: 0.4562\n",
      "Epoch 2/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 574ms/step - accuracy: 0.5531 - loss: 0.4531 - val_accuracy: 0.6017 - val_loss: 0.4187\n",
      "Epoch 3/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 572ms/step - accuracy: 0.6017 - loss: 0.4208 - val_accuracy: 0.6157 - val_loss: 0.4114\n",
      "Epoch 4/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 678ms/step - accuracy: 0.6068 - loss: 0.4091 - val_accuracy: 0.6271 - val_loss: 0.4030\n",
      "Epoch 5/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 762ms/step - accuracy: 0.6291 - loss: 0.4010 - val_accuracy: 0.6498 - val_loss: 0.3804\n",
      "Epoch 6/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 782ms/step - accuracy: 0.6193 - loss: 0.3981 - val_accuracy: 0.6611 - val_loss: 0.3746\n",
      "Epoch 7/7\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 613ms/step - accuracy: 0.6498 - loss: 0.3816 - val_accuracy: 0.6472 - val_loss: 0.3813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x213b033f410>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, imdb_embedding_dim, embeddings_initializer=Constant(\n",
    "    imdb_embedding_matrix), trainable=False))\n",
    "model.add(LSTM(192, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(len(imdb_mlb.classes_), activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(imdb_X_train, imdb_y_train, batch_size=64, epochs=7, validation_data=(\n",
    "    imdb_X_test, imdb_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "imdb_y_pred = model.predict(imdb_X_test)\n",
    "imdb_y_pred_classes = (imdb_y_pred > 0.5).astype(int) #0.5 here means that the estimated probability must be over 0.5. Otherwise, it becomes None (has a big impact on precision/recall but is usually a trade-off between the two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5318777292576419\n",
      "Precision: 0.6880630630630631\n",
      "Recall: 0.5336244541484716\n",
      "F1-score: 0.6010821446138711\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(cmu_y_test, cmu_y_pred_classes)\n",
    "precision = precision_score(cmu_y_test, cmu_y_pred_classes, average='micro')\n",
    "recall = recall_score(cmu_y_test, cmu_y_pred_classes, average='micro')\n",
    "f1 = f1_score(cmu_y_test, cmu_y_pred_classes, average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: Comedy\n",
      "Accuracy: 0.8296943231441049\n",
      "Precision: 0.7027027027027027\n",
      "Recall: 0.6066666666666667\n",
      "F1-score: 0.6511627906976745\n",
      "\n",
      "Genre: Drama\n",
      "Accuracy: 0.8087336244541484\n",
      "Precision: 0.6907216494845361\n",
      "Recall: 0.45733788395904434\n",
      "F1-score: 0.5503080082135524\n",
      "\n",
      "Genre: Horror\n",
      "Accuracy: 0.874235807860262\n",
      "Precision: 0.6845425867507886\n",
      "Recall: 0.8314176245210728\n",
      "F1-score: 0.7508650519031141\n",
      "\n",
      "Genre: Thriller\n",
      "Accuracy: 0.7790393013100436\n",
      "Precision: 0.6610169491525424\n",
      "Recall: 0.26804123711340205\n",
      "F1-score: 0.38141809290953543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for each genre\n",
    "genre_scores = {}\n",
    "for i, genre in enumerate(cmu_mlb.classes_):\n",
    "    genre_accuracy = accuracy_score(cmu_y_test[:, i], cmu_y_pred_classes[:, i])\n",
    "    genre_precision = precision_score(cmu_y_test[:, i], cmu_y_pred_classes[:, i])\n",
    "    genre_recall = recall_score(cmu_y_test[:, i], cmu_y_pred_classes[:, i])\n",
    "    genre_f1 = f1_score(cmu_y_test[:, i], cmu_y_pred_classes[:, i])\n",
    "    \n",
    "    genre_scores[genre] = {'Accuracy': genre_accuracy,\n",
    "                           'Precision': genre_precision,\n",
    "                           'Recall': genre_recall,\n",
    "                           'F1-score': genre_f1}\n",
    "\n",
    "# Print scores for each genre\n",
    "for genre, scores in genre_scores.items():\n",
    "    print(f'Genre: {genre}')\n",
    "    print(f'Accuracy: {scores[\"Accuracy\"]}')\n",
    "    print(f'Precision: {scores[\"Precision\"]}')\n",
    "    print(f'Recall: {scores[\"Recall\"]}')\n",
    "    print(f'F1-score: {scores[\"F1-score\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5563318777292576\n",
      "Precision: 0.7065337763012182\n",
      "Recall: 0.5572052401746724\n",
      "F1-score: 0.623046875\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(imdb_y_test, imdb_y_pred_classes)\n",
    "precision = precision_score(imdb_y_test, imdb_y_pred_classes, average='micro')\n",
    "recall = recall_score(imdb_y_test, imdb_y_pred_classes, average='micro')\n",
    "f1 = f1_score(imdb_y_test, imdb_y_pred_classes, average='micro')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: Comedy\n",
      "Accuracy: 0.8078602620087336\n",
      "Precision: 0.6169590643274854\n",
      "Recall: 0.7033333333333334\n",
      "F1-score: 0.6573208722741433\n",
      "\n",
      "Genre: Drama\n",
      "Accuracy: 0.8148471615720524\n",
      "Precision: 0.7454545454545455\n",
      "Recall: 0.4197952218430034\n",
      "F1-score: 0.537117903930131\n",
      "\n",
      "Genre: Horror\n",
      "Accuracy: 0.879475982532751\n",
      "Precision: 0.8153846153846154\n",
      "Recall: 0.6091954022988506\n",
      "F1-score: 0.6973684210526315\n",
      "\n",
      "Genre: Thriller\n",
      "Accuracy: 0.8235807860262009\n",
      "Precision: 0.7213930348258707\n",
      "Recall: 0.49828178694158076\n",
      "F1-score: 0.5894308943089431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for each genre\n",
    "imdb_genre_scores = {}\n",
    "for i, genre in enumerate(imdb_mlb.classes_):\n",
    "    genre_accuracy = accuracy_score(imdb_y_test[:, i], imdb_y_pred_classes[:, i])\n",
    "    genre_precision = precision_score(imdb_y_test[:, i], imdb_y_pred_classes[:, i])\n",
    "    genre_recall = recall_score(imdb_y_test[:, i], imdb_y_pred_classes[:, i])\n",
    "    genre_f1 = f1_score(imdb_y_test[:, i], imdb_y_pred_classes[:, i])\n",
    "    \n",
    "    imdb_genre_scores[genre] = {'Accuracy': genre_accuracy,\n",
    "                                'Precision': genre_precision,\n",
    "                                'Recall': genre_recall,\n",
    "                                'F1-score': genre_f1}\n",
    "\n",
    "# Print scores for each genre\n",
    "for genre, scores in imdb_genre_scores.items():\n",
    "    print(f'Genre: {genre}')\n",
    "    print(f'Accuracy: {scores[\"Accuracy\"]}')\n",
    "    print(f'Precision: {scores[\"Precision\"]}')\n",
    "    print(f'Recall: {scores[\"Recall\"]}')\n",
    "    print(f'F1-score: {scores[\"F1-score\"]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
