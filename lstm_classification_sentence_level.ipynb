{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Genre Classification with LSTM\n",
    "## Sentence-level approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "RANDOM_STATE = 1212\n",
    "\n",
    "cmu_data = pd.read_csv('cmu_data_final.csv')\n",
    "imdb_data = pd.read_csv('imdb_data_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and (token.isalnum() or token == '.')]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "cmu_data['processed_plot'] = cmu_data['plot'].apply(preprocess_text)\n",
    "imdb_data['processed_plot'] = imdb_data['plot'].apply(preprocess_text)\n",
    "\n",
    "# Choose either the CMU embeddings or the Google News embeddings\n",
    "\n",
    "# Word2vec embeddings, trained on the CMU dataset\n",
    "# word2vec_model = KeyedVectors.load('word2vec_model_from_cmu_utf8.bin')\n",
    "\n",
    "# Google news word2vec embeddings\n",
    "import gensim.downloader as api\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad the text sequences\n",
    "cmu_tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "cmu_tokenizer.fit_on_texts(cmu_data['processed_plot'])\n",
    "cmu_sequences = cmu_tokenizer.texts_to_sequences(cmu_data['processed_plot'])\n",
    "cmu_word_index = cmu_tokenizer.word_index\n",
    "cmu_padded_sequences = pad_sequences(cmu_sequences, maxlen=300)\n",
    "\n",
    "# Label encoding\n",
    "cmu_data['genre'] = cmu_data['genre'].apply(lambda x: x.split('|'))\n",
    "cmu_mlb = MultiLabelBinarizer()\n",
    "cmu_genres_encoded = cmu_mlb.fit_transform(cmu_data['genre'])\n",
    "\n",
    "# Train-test split\n",
    "cmu_X_train, cmu_X_test, cmu_y_train, cmu_y_test = train_test_split(\n",
    "    cmu_padded_sequences, cmu_genres_encoded, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and pad the text sequences\n",
    "imdb_tokenizer = Tokenizer(filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "imdb_tokenizer.fit_on_texts(imdb_data['processed_plot'])\n",
    "imdb_sequences = imdb_tokenizer.texts_to_sequences(imdb_data['processed_plot'])\n",
    "imdb_word_index = imdb_tokenizer.word_index\n",
    "imdb_padded_sequences = pad_sequences(imdb_sequences, maxlen=300)\n",
    "\n",
    "# Label encoding\n",
    "imdb_data['genre'] = imdb_data['genre'].apply(lambda x: x.split('|'))\n",
    "imdb_mlb = MultiLabelBinarizer()\n",
    "imdb_genres_encoded = imdb_mlb.fit_transform(imdb_data['genre'])\n",
    "\n",
    "# Train-test split\n",
    "imdb_X_train, imdb_X_test, imdb_y_train, imdb_y_test = train_test_split(\n",
    "    imdb_padded_sequences, imdb_genres_encoded, test_size=0.2, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CMU embedding matrix\n",
    "\"\"\" cmu_embedding_dim = word2vec_model.vector_size\n",
    "cmu_embedding_matrix = np.zeros((len(cmu_word_index) + 1, cmu_embedding_dim))\n",
    "for word, i in cmu_word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        cmu_embedding_matrix[i] = word2vec_model.wv[word] \"\"\"\n",
    "\n",
    "# Create the Google News embedding matrix\n",
    "cmu_embedding_dim = word2vec_model.vector_size\n",
    "cmu_embedding_matrix = np.zeros((len(cmu_word_index) + 1, cmu_embedding_dim))\n",
    "for word, i in cmu_word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        cmu_embedding_matrix[i] = word2vec_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(cmu_word_index) + 1, cmu_embedding_dim, embeddings_initializer=Constant(\n",
    "    cmu_embedding_matrix), trainable=False))\n",
    "model.add(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(len(cmu_mlb.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(cmu_X_train, cmu_y_train, batch_size=64, epochs=7, validation_data=(\n",
    "    cmu_X_test, cmu_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences back to text\n",
    "cmu_X_train_reverted = cmu_tokenizer.sequences_to_texts(cmu_X_test)\n",
    "\n",
    "sentencecount = 0\n",
    "plotcount = 0\n",
    "cmu_sentence_level_predictions = []\n",
    "\n",
    "\n",
    "for plot_summary in cmu_X_train_reverted:\n",
    "\n",
    "    sentences = [sentence.strip() for sentence in plot_summary.split(\".\") if sentence.strip()]\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    score3 = 0\n",
    "    score4 = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "\n",
    "        sequence = cmu_tokenizer.texts_to_sequences([sentence])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=300)\n",
    "\n",
    "        prediction = model.predict(padded_sequence)\n",
    "\n",
    "        score1 += prediction[0][0]\n",
    "        score2 += prediction[0][1]\n",
    "        score3 += prediction[0][2]\n",
    "        score4 += prediction[0][3]\n",
    "    plotcount += 1\n",
    "\n",
    "    processed_score1 = score1 / num_sentences\n",
    "    processed_score2 = score2 / num_sentences\n",
    "    processed_score3 = score3 / num_sentences\n",
    "    processed_score4 = score4 / num_sentences\n",
    "    processed_values = [processed_score1,processed_score2,processed_score3,processed_score4]\n",
    "\n",
    "    cmu_sentence_level_predictions.append(processed_values)\n",
    "    print(plotcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CMU embedding matrix\n",
    "\"\"\" imdb_embedding_dim = word2vec_model.vector_size\n",
    "imdb_embedding_matrix = np.zeros((len(imdb_word_index) + 1, imdb_embedding_dim))\n",
    "for word, i in imdb_word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        imdb_embedding_matrix[i] = word2vec_model.wv[word] \"\"\"\n",
    "\n",
    "# Create the Google News embedding matrix\n",
    "imdb_embedding_dim = word2vec_model.vector_size\n",
    "imdb_embedding_matrix = np.zeros((len(imdb_word_index) + 1, imdb_embedding_dim))\n",
    "for word, i in imdb_word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        imdb_embedding_matrix[i] = word2vec_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compile the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(imdb_word_index) + 1, imdb_embedding_dim, embeddings_initializer=Constant(\n",
    "    imdb_embedding_matrix), trainable=False))\n",
    "model.add(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(len(imdb_mlb.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(imdb_X_train, imdb_y_train, batch_size=64, epochs=7, validation_data=(\n",
    "    imdb_X_test, imdb_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequences back to text\n",
    "imdb_X_train_reverted = imdb_tokenizer.sequences_to_texts(imdb_X_test)\n",
    "\n",
    "sentencecount = 0\n",
    "plotcount = 0\n",
    "imdb_sentence_level_predictions = []\n",
    "\n",
    "for plot_summary in imdb_X_train_reverted:\n",
    "    sentences = [sentence.strip() for sentence in plot_summary.split(\".\") if sentence.strip()]\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    score3 = 0\n",
    "    score4 = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sequence = imdb_tokenizer.texts_to_sequences([sentence])\n",
    "        padded_sequence = pad_sequences(sequence, maxlen=300)\n",
    "\n",
    "        prediction = model.predict(padded_sequence)\n",
    "\n",
    "        score1 += prediction[0][0]\n",
    "        score2 += prediction[0][1]\n",
    "        score3 += prediction[0][2]\n",
    "        score4 += prediction[0][3]\n",
    "\n",
    "    processed_score1 = score1 / num_sentences\n",
    "    processed_score2 = score2 / num_sentences\n",
    "    processed_score3 = score3 / num_sentences\n",
    "    processed_score4 = score4 / num_sentences\n",
    "    processed_values = [processed_score1,processed_score2,processed_score3,processed_score4]\n",
    "\n",
    "    imdb_sentence_level_predictions.append(processed_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_y_pred_classes_sentence = []\n",
    "\n",
    "for prediction_values in cmu_sentence_level_predictions:\n",
    "    prediction_values = np.array(prediction_values)\n",
    "    \n",
    "    # Apply the threshold and convert to int\n",
    "    prediction_class = (prediction_values > 0.35).astype(int)\n",
    "    \n",
    "    cmu_y_pred_classes_sentence.append(prediction_class)\n",
    "\n",
    "cmu_y_pred_classes_sentence = np.array(cmu_y_pred_classes_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "precision_micro = precision_score(cmu_y_test, cmu_y_pred_classes_sentence, average='micro')\n",
    "recall_micro = recall_score(cmu_y_test, cmu_y_pred_classes_sentence, average='micro')\n",
    "f1_micro = f1_score(cmu_y_test, cmu_y_pred_classes_sentence, average='micro')\n",
    "\n",
    "precision_macro = precision_score(cmu_y_test, cmu_y_pred_classes_sentence, average='macro')\n",
    "recall_macro = recall_score(cmu_y_test, cmu_y_pred_classes_sentence, average='macro')\n",
    "f1_macro = f1_score(cmu_y_test, cmu_y_pred_classes_sentence, average='macro')\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Micro Precision: {precision_micro}')\n",
    "print(f'Micro Recall: {recall_micro}')\n",
    "print(f'Micro F1-score: {f1_micro}')\n",
    "print()\n",
    "print(f'Macro Precision: {precision_macro}')\n",
    "print(f'Macro Recall: {recall_macro}')\n",
    "print(f'Macro F1-score: {f1_macro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for each genre\n",
    "cmu_genre_scores = {}\n",
    "for i, genre in enumerate(cmu_mlb.classes_):\n",
    "    genre_accuracy = accuracy_score(cmu_y_test[:, i], cmu_y_pred_classes_sentence[:, i])\n",
    "    genre_precision = precision_score(cmu_y_test[:, i], cmu_y_pred_classes_sentence[:, i])\n",
    "    genre_recall = recall_score(cmu_y_test[:, i], cmu_y_pred_classes_sentence[:, i])\n",
    "    genre_f1 = f1_score(cmu_y_test[:, i], cmu_y_pred_classes_sentence[:, i])\n",
    "    \n",
    "    cmu_genre_scores[genre] = {'Accuracy': genre_accuracy,\n",
    "                                'Precision': genre_precision,\n",
    "                                'Recall': genre_recall,\n",
    "                                'F1-score': genre_f1}\n",
    "\n",
    "# Print scores for each genre\n",
    "for genre, scores in cmu_genre_scores.items():\n",
    "    print(f'Genre: {genre}')\n",
    "    print(f'Accuracy: {scores[\"Accuracy\"]}')\n",
    "    print(f'Precision: {scores[\"Precision\"]}')\n",
    "    print(f'Recall: {scores[\"Recall\"]}')\n",
    "    print(f'F1-score: {scores[\"F1-score\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_y_pred_classes_sentence = []\n",
    "\n",
    "for prediction_values in imdb_sentence_level_predictions:\n",
    "    prediction_values = np.array(prediction_values)\n",
    "    \n",
    "    # Apply the threshold and convert to int\n",
    "    prediction_class = (prediction_values > 0.35).astype(int)\n",
    "    \n",
    "    imdb_y_pred_classes_sentence.append(prediction_class)\n",
    "\n",
    "imdb_y_pred_classes_sentence = np.array(imdb_y_pred_classes_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "precision_micro = precision_score(imdb_y_test, imdb_y_pred_classes_sentence, average='micro')\n",
    "recall_micro = recall_score(imdb_y_test, imdb_y_pred_classes_sentence, average='micro')\n",
    "f1_micro = f1_score(imdb_y_test, imdb_y_pred_classes_sentence, average='micro')\n",
    "\n",
    "precision_macro = precision_score(imdb_y_test, imdb_y_pred_classes_sentence, average='macro')\n",
    "recall_macro = recall_score(imdb_y_test, imdb_y_pred_classes_sentence, average='macro')\n",
    "f1_macro = f1_score(imdb_y_test, imdb_y_pred_classes_sentence, average='macro')\n",
    "\n",
    "print(f'Precision (micro): {precision_micro}')\n",
    "print(f'Recall (micro): {recall_micro}')\n",
    "print(f'F1-score (micro): {f1_micro}')\n",
    "print()\n",
    "print(f'Precision (macro): {precision_macro}')\n",
    "print(f'Recall (macro): {recall_macro}')\n",
    "print(f'F1-score (macro): {f1_macro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics for each genre\n",
    "imdb_genre_scores = {}\n",
    "for i, genre in enumerate(imdb_mlb.classes_):\n",
    "    genre_accuracy = accuracy_score(imdb_y_test[:, i], imdb_y_pred_classes_sentence[:, i])\n",
    "    genre_precision = precision_score(imdb_y_test[:, i], imdb_y_pred_classes_sentence[:, i])\n",
    "    genre_recall = recall_score(imdb_y_test[:, i], imdb_y_pred_classes_sentence[:, i])\n",
    "    genre_f1 = f1_score(imdb_y_test[:, i], imdb_y_pred_classes_sentence[:, i])\n",
    "    \n",
    "    imdb_genre_scores[genre] = {'Accuracy': genre_accuracy,\n",
    "                                'Precision': genre_precision,\n",
    "                                'Recall': genre_recall,\n",
    "                                'F1-score': genre_f1}\n",
    "\n",
    "# Print scores for each genre\n",
    "for genre, scores in imdb_genre_scores.items():\n",
    "    print(f'Genre: {genre}')\n",
    "    print(f'Accuracy: {scores[\"Accuracy\"]}')\n",
    "    print(f'Precision: {scores[\"Precision\"]}')\n",
    "    print(f'Recall: {scores[\"Recall\"]}')\n",
    "    print(f'F1-score: {scores[\"F1-score\"]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
