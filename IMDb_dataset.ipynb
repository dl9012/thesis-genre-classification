{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "api_key1 = \"b58bb13f\"\n",
    "api_key2 = \"9bf3218a\"\n",
    "\n",
    "RANDOM_STATE = 1212\n",
    "\n",
    "##\n",
    "asd = pd.DataFrame(columns=[\"Title\", \"Year\", \"Genre\", \"Plot\"])\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' not_found = pd.DataFrame(columns=[\"movie_name\", \"year\", \"genre\", \"plot\"])\\nimdb_data = pd.DataFrame(columns=[\"movie_name\", \"year\", \"genre\", \"plot\"])\\n\\nnot_found.to_csv(\"not_found_NEW.csv\", index=False)\\nimdb_data.to_csv(\"imdb_data_NEW.csv\", index=False) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This block can be run to create the necessary csv files for the first time\n",
    "\n",
    "\"\"\" not_found = pd.DataFrame(columns=[\"movie_name\", \"year\", \"genre\", \"plot\"])\n",
    "imdb_data = pd.DataFrame(columns=[\"movie_name\", \"year\", \"genre\", \"plot\"])\n",
    "\n",
    "not_found.to_csv(\"not_found_NEW.csv\", index=False)\n",
    "imdb_data.to_csv(\"imdb_data_NEW.csv\", index=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request limit reached!\n"
     ]
    }
   ],
   "source": [
    "not_found = pd.read_csv(\"not_found_NEW.csv\")\n",
    "imdb_data = pd.read_csv(\"imdb_data_NEW.csv\")\n",
    "cmu_data = pd.read_csv(\"cmu_backup.csv\")\n",
    "\n",
    "for index, row in cmu_data.iterrows():\n",
    "    if len(imdb_data[imdb_data[\"genre\"] == row[\"genre\"]]) >= 1500:\n",
    "        continue\n",
    "\n",
    "    if row[\"movie_name\"] in not_found[\"movie_name\"].values and row[\"year\"] in not_found[\"year\"].values and row[\"genre\"] in not_found[\"genre\"].values:\n",
    "        continue\n",
    "    if row[\"movie_name\"] in imdb_data[\"movie_name\"].values and row[\"year\"] in imdb_data[\"year\"].values and row[\"genre\"] in imdb_data[\"genre\"].values:\n",
    "        continue\n",
    "\n",
    "    url = f\"http://www.omdbapi.com/?t={row['movie_name']}&y={row['year']}&plot=full&apikey={api_key2}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if data[\"Response\"] == \"False\":\n",
    "        if data[\"Error\"] == \"Request limit reached!\":\n",
    "            print(data[\"Error\"])\n",
    "            break\n",
    "        if data[\"Error\"] == \"Movie not found!\":\n",
    "            url = f\"http://www.omdbapi.com/?t={row['movie_name']}&y={int(row['year']) - 1}&plot=full&apikey={api_key1}\"            \n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            time.sleep(0.5)\n",
    "            if data[\"Response\"] == \"True\":\n",
    "                if data[\"Plot\"].count(\".\") >= 2:\n",
    "                    imdb_data.loc[len(imdb_data)] = [data['Title'], int(data['Year'][:4]), row['genre'], data['Plot']]\n",
    "                else:\n",
    "                    not_found.loc[len(not_found)] = [row[\"movie_name\"], int(row[\"year\"]), row['genre'], data['Plot']]\n",
    "            else:\n",
    "                not_found.loc[len(not_found)] = [row[\"movie_name\"], int(row[\"year\"]), row['genre'], row['plot']]\n",
    "        else: \n",
    "            not_found.loc[len(not_found)] = [row[\"movie_name\"], int(row[\"year\"]), row['genre'], row['plot']]\n",
    "    else:\n",
    "        if data[\"Plot\"].count(\".\") >= 2:\n",
    "            imdb_data.loc[len(imdb_data)] = [data['Title'], int(data['Year'][:4]), row['genre'], data['Plot']]\n",
    "        else:\n",
    "            not_found.loc[len(not_found)] = [row[\"movie_name\"], int(row[\"year\"]), row['genre'], data['Plot']]\n",
    "\n",
    "\n",
    "not_found.to_csv(\"not_found_NEW.csv\", index=False)\n",
    "imdb_data.to_csv(\"imdb_data_NEW.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing movies that were not found from the CMU dataset and filling it back up\n",
    "\n",
    "## WIP, don't run these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Merge the dataframes on movie_name and year\\nmerged = pd.merge(cmu_data, not_found, on=[\\'movie_name\\', \\'year\\', \\'genre\\'], how=\\'left\\', indicator=True)\\n\\n# Filter out rows where movie_name and year match\\nfiltered_data = merged[merged[\\'_merge\\'] == \\'left_only\\'].drop(columns=[\\'_merge\\'])\\n\\n# get the genres that were removed\\ngenres = not_found[\"genre\"].unique()\\n\\n# get the genres that were removed and the number of items in each genre\\ngenre_counts = not_found[\"genre\"].value_counts()\\n\\nprint(genre_counts)\\n\\n\\n\\n\\n# merged.shape, filtered_data.shape '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Merge the dataframes on movie_name and year\n",
    "merged = pd.merge(cmu_data, not_found, on=['movie_name', 'year', 'genre'], how='left', indicator=True)\n",
    "\n",
    "# Filter out rows where movie_name and year match\n",
    "filtered_data = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "# get the genres that were removed\n",
    "genres = not_found[\"genre\"].unique()\n",
    "\n",
    "# get the genres that were removed and the number of items in each genre\n",
    "genre_counts = not_found[\"genre\"].value_counts()\n",
    "\n",
    "print(genre_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# merged.shape, filtered_data.shape \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' comedy_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Comedy\")]\\nhorror_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Horror\")]\\nthriller_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Thriller\")]\\ndrama_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Drama\")]\\n\\n# add 1500 items to each genre\\n# if the genre has less than 1500 items, add items from cmu_backups\\n# if adding a comedy movie, make sure it has no horror, thriller or drama in the genre\\n\\nfiltered_comedy = cmu_backups[cmu_backups[\\'genre\\'].apply(lambda x: \\'Comedy\\' in x and not any(genre in x for genre in [\\'Horror\\', \\'Thriller\\', \\'Drama\\']))]\\ncomedy_movies = pd.concat([comedy_movies, filtered_comedy])\\ncomedy_movies = comedy_movies.head(1500)\\n\\nfiltered_horror = cmu_backups[cmu_backups[\\'genre\\'].apply(lambda x: \\'Horror\\' in x and not any(genre in x for genre in [\\'Comedy\\', \\'Thriller\\', \\'Drama\\']))]\\nhorror_movies = pd.concat([horror_movies, filtered_horror])\\nhorror_movies = horror_movies.head(1500)\\n\\nfiltered_thriller = cmu_backups[cmu_backups[\\'genre\\'].apply(lambda x: \\'Thriller\\' in x and not any(genre in x for genre in [\\'Comedy\\', \\'Horror\\', \\'Drama\\']))]\\nthriller_movies = pd.concat([thriller_movies, filtered_thriller])\\nthriller_movies = thriller_movies.head(1500)\\n\\nfiltered_drama = cmu_backups[cmu_backups[\\'genre\\'].apply(lambda x: \\'Drama\\' in x and not any(genre in x for genre in [\\'Comedy\\', \\'Horror\\', \\'Thriller\\']))]\\ndrama_movies = pd.concat([drama_movies, filtered_drama])\\ndrama_movies = drama_movies.head(1500)\\n\\ncmu_dataset = pd.concat([comedy_movies, horror_movies, thriller_movies, drama_movies])\\ncmu_dataset.reset_index(drop=True, inplace=True)\\n\\ncmu_backups = cmu_backups[~cmu_backups.isin(cmu_dataset)].dropna()\\n\\ncmu_dataset.to_csv(\"cmu_dataset_v2.csv\", index=False)\\ncmu_backups.to_csv(\"cmu_movies_backup.csv\", index=False)\\n\\ncmu_dataset.shape, cmu_backups.shape, comedy_movies.shape, horror_movies.shape, thriller_movies.shape, drama_movies.shape '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" comedy_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Comedy\")]\n",
    "horror_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Horror\")]\n",
    "thriller_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Thriller\")]\n",
    "drama_movies = filtered_data[filtered_data[\"genre\"].str.contains(\"Drama\")]\n",
    "\n",
    "# add 1500 items to each genre\n",
    "# if the genre has less than 1500 items, add items from cmu_backups\n",
    "# if adding a comedy movie, make sure it has no horror, thriller or drama in the genre\n",
    "\n",
    "filtered_comedy = cmu_backups[cmu_backups['genre'].apply(lambda x: 'Comedy' in x and not any(genre in x for genre in ['Horror', 'Thriller', 'Drama']))]\n",
    "comedy_movies = pd.concat([comedy_movies, filtered_comedy])\n",
    "comedy_movies = comedy_movies.head(1500)\n",
    "\n",
    "filtered_horror = cmu_backups[cmu_backups['genre'].apply(lambda x: 'Horror' in x and not any(genre in x for genre in ['Comedy', 'Thriller', 'Drama']))]\n",
    "horror_movies = pd.concat([horror_movies, filtered_horror])\n",
    "horror_movies = horror_movies.head(1500)\n",
    "\n",
    "filtered_thriller = cmu_backups[cmu_backups['genre'].apply(lambda x: 'Thriller' in x and not any(genre in x for genre in ['Comedy', 'Horror', 'Drama']))]\n",
    "thriller_movies = pd.concat([thriller_movies, filtered_thriller])\n",
    "thriller_movies = thriller_movies.head(1500)\n",
    "\n",
    "filtered_drama = cmu_backups[cmu_backups['genre'].apply(lambda x: 'Drama' in x and not any(genre in x for genre in ['Comedy', 'Horror', 'Thriller']))]\n",
    "drama_movies = pd.concat([drama_movies, filtered_drama])\n",
    "drama_movies = drama_movies.head(1500)\n",
    "\n",
    "cmu_dataset = pd.concat([comedy_movies, horror_movies, thriller_movies, drama_movies])\n",
    "cmu_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cmu_backups = cmu_backups[~cmu_backups.isin(cmu_dataset)].dropna()\n",
    "\n",
    "cmu_dataset.to_csv(\"cmu_dataset_v2.csv\", index=False)\n",
    "cmu_backups.to_csv(\"cmu_movies_backup.csv\", index=False)\n",
    "\n",
    "cmu_dataset.shape, cmu_backups.shape, comedy_movies.shape, horror_movies.shape, thriller_movies.shape, drama_movies.shape \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
